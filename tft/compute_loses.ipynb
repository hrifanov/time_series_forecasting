{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086a400b-730b-443c-a150-9356da96c5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "import logging\n",
    "logging.disable(logging.CRITICAL)\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.nn import MSELoss, CrossEntropyLoss\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor, ModelCheckpoint\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import optuna\n",
    "from optuna.integration import PyTorchLightningPruningCallback\n",
    "\n",
    "from darts import TimeSeries, concatenate\n",
    "from darts.models import NHiTSModel\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "from darts.metrics import smape, rmse\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db7dac3-a2a7-418e-be86-0e57b5c6c808",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.backends.mps.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48299ecc-85cf-44c8-a11d-4c19c21d3c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../utils/preprocessing.ipynb\n",
    "%run ../utils/losses.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c4c057-14ea-4f1b-9a7b-e49d604104d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mps_device = torch.device(\"mps\")\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    accelerator=\"mps\"\n",
    "else:\n",
    "    print (\"MPS device not found.\")\n",
    "    accelerator=\"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e996e8-c5a5-4898-8d06-5824cd0086d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(model, scaled_splits_data, scaled_full_data, input_len, output_len, limit=None):\n",
    "    range_len = len(scaled_splits_data['scaled_y_test'])\n",
    "    predictions = []\n",
    "    predictions_count = 0\n",
    "\n",
    "    for i in range(0, range_len, output_len):\n",
    "        beginning_idx = len(scaled_splits_data['scaled_y_train']) + len(scaled_splits_data['scaled_y_val']) - input_len + i\n",
    "        end_idx = len(scaled_splits_data['scaled_y_train']) + len(scaled_splits_data['scaled_y_val']) + i\n",
    "\n",
    "        pred = model.predict(\n",
    "            n=output_len,\n",
    "            series=scaled_full_data['scaled_y_full'][beginning_idx:end_idx],\n",
    "            past_covariates=scaled_full_data['scaled_X_full'][beginning_idx:end_idx],\n",
    "            n_jobs=-1,\n",
    "            verbose=False,\n",
    "        )\n",
    "        \n",
    "        predictions.append(pred)\n",
    "        predictions_count += 1\n",
    "        \n",
    "        if limit is not None and predictions_count >= limit:\n",
    "            break\n",
    "\n",
    "    individual_pred = concatenate(predictions)\n",
    "    return individual_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07443e49-99cf-450b-9796-de6d53bce419",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_actual(actual, prediction):    \n",
    "    dfY = pd.DataFrame()\n",
    "    dfY[[\"mid_close\", 'spread', 'vol']] = TimeSeries.pd_dataframe(actual)\n",
    "    dfY[\"prediction\"] = TimeSeries.pd_series(prediction)\n",
    "    \n",
    "    min_vol, max_vol = dfY['vol'].min(), dfY['vol'].max()\n",
    "    dfY['normalized_volume'] = (dfY['vol'] - min_vol) / (max_vol - min_vol)\n",
    "\n",
    "        \n",
    "    return dfY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8ac59e-c998-466e-9092-77df9951f91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "TICKERS = ['QCOM', 'NVDA', 'AMZN', 'MSFT', 'GOOG', 'TSLA', 'AMD', 'INTC', 'NFLX', 'BAC', 'WFC', 'GS', 'MA', 'SQ', 'PYPL']\n",
    "FREQUENCIES = [15, 5]\n",
    "\n",
    "#TICKERS = ['QCOM']\n",
    "#FREQUENCIES = [15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e411d61-aa12-4535-83ec-7083141be283",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "for frequency in FREQUENCIES:\n",
    "    for ticker in TICKERS:\n",
    "\n",
    "        DATA_FREQUENCY = minute_frequencies_conventions[frequency]\n",
    "        MODEL_NAME = f'{ticker}_{frequency}_TFT'\n",
    "        \n",
    "        OUTPUT_LEN = 1\n",
    "        INPUT_LEN = 70 if frequency == 15 else 270\n",
    "\n",
    "        \n",
    "        model = NHiTSModel.load_from_checkpoint(MODEL_NAME, work_dir=f'/Users/work/repos/diplomka/tft/saved_models/{frequency}/')\n",
    "\n",
    "        stock = load_stock_data(f'../data/resampled_data/{DATA_FREQUENCY}/{ticker}_resampled_{DATA_FREQUENCY}.csv', frequency)\n",
    "        \n",
    "        X_y_df = separate(stock)\n",
    "        \n",
    "        splits = split_data(**X_y_df)\n",
    "        \n",
    "        ts_splits = transform_splits_to_time_series(**splits)\n",
    "        \n",
    "        ts_full = transform_to_time_series(**X_y_df)\n",
    "        \n",
    "        scaled_splits_data = scale_splits_data(**ts_splits)\n",
    "        \n",
    "        scaled_full_data = scale_full_data(ts_full['ts_X_full'], ts_full['ts_y_full'], scaled_splits_data['scaler_X'], scaled_splits_data['scaler_y'])\n",
    "        \n",
    "        stock_full = {\n",
    "            \"ticker\": ticker,\n",
    "            \"stock\": stock,\n",
    "            \"splits\": splits,\n",
    "            \"ts_splits\": ts_splits,\n",
    "            \"ts_full\": ts_full,\n",
    "            \"scaled_splits_data\": scaled_splits_data,\n",
    "            \"scaled_full_data\": scaled_full_data\n",
    "        }\n",
    "\n",
    "\n",
    "        individual_pred = make_predictions(model, scaled_splits_data, scaled_full_data, INPUT_LEN, OUTPUT_LEN);\n",
    "        pred_unscaled = scaled_splits_data['scaler_y'].inverse_transform(individual_pred)\n",
    "\n",
    "        actual = stock_full['ts_splits']['ts_X_test'][['close', 'spread', 'vol']]\n",
    "        dfY = pred_actual(actual, pred_unscaled)\n",
    "\n",
    "        dfY.to_csv(f'saved_models/{frequency}/{MODEL_NAME}/backtesting_dataftame.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DP (Python 3.11)",
   "language": "python",
   "name": "dp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
